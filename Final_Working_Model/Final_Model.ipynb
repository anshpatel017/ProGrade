{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4b9413-d736-4d19-b49b-a67caea50c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifacts (if available)...\n",
      "  - loaded tech_stack_classifier.joblib\n",
      "  - loaded mlb.joblib\n",
      "  - loaded tech_domain_classifier.joblib\n",
      "  - loaded domain_labels.joblib\n",
      "  - loaded quality_model.joblib\n",
      "  - failed to load scored_dataset.csv: pop from empty list\n",
      "Loaded prograde_model from prograde_model.joblib (type: <class 'prograde_model.ProGrade'>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a GitHub repo URL (https.. or git@.. or local path):\n",
      ">  https://github.com/rajank18/ScanX_frontend.git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning (shallow) to temporary folder: C:\\Users\\odcha\\AppData\\Local\\Temp\\prograde_nb_clone_l5piaddc\n",
      "\n",
      "Running heuristic analyze_repository(...) (ProGrade core)...\n",
      "\n",
      "Calling model.analyze(...) from prograde_model.joblib...\n",
      "Model analyze returned successfully.\n",
      "\n",
      "Running ML predictions (quality + domain) using separate artifacts...\n",
      "ML predictions ready.\n",
      "\n",
      "==================================================\n",
      "ðŸš€ Analysis Report for: https://github.com/rajank18/ScanX_frontend.git\n",
      "==================================================\n",
      "\n",
      "Repo Name: prograde_nb_clone_l5piaddc\n",
      "\n",
      "Tech Stack (Detected):\n",
      "  --- Languages ---\n",
      "    - CSS\n",
      "    - HTML\n",
      "    - JavaScript\n",
      "  --- Frameworks ---\n",
      "    - React\n",
      "\n",
      "Domain (Detected): Frontend Web Development\n",
      "\n",
      "Scores:\n",
      "  - Code Quality: 2.00\n",
      "  - Comment Management: 1.00\n",
      "  - Documentation: 3.00\n",
      "  - Contribution: 4.00\n",
      "  - Overall: 2.50\n",
      "\n",
      "Top Contributors:\n",
      "  - rajank18: 9 commits\n",
      "\n",
      "==================================================\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, tempfile, shutil, stat, subprocess, gc\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import git\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ensure prograde_core is importable (it contains analyze_repository)\n",
    "try:\n",
    "    import prograde_core\n",
    "    analyze_repository = prograde_core.analyze_repository\n",
    "except Exception as e:\n",
    "    print(\"Error: could not import prograde_core.analyze_repository:\", e)\n",
    "    raise\n",
    "\n",
    "# Paths (adjust if files are elsewhere)\n",
    "PROGRADE_MODEL_PATH = \"prograde_model.joblib\"\n",
    "ARTIFACTS = {\n",
    "    \"stack_model\": \"tech_stack_classifier.joblib\",\n",
    "    \"mlb\": \"mlb.joblib\",\n",
    "    \"domain_model\": \"tech_domain_classifier.joblib\",\n",
    "    \"domain_labels\": \"domain_labels.joblib\",\n",
    "    \"quality_model\": \"quality_model.joblib\",\n",
    "    \"schema_csv\": \"scored_dataset.csv\"\n",
    "}\n",
    "\n",
    "# --- Utilities ---\n",
    "def safe_load(path):\n",
    "    try:\n",
    "        return joblib.load(path)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def git_commit_count(repo_path):\n",
    "    try:\n",
    "        out = subprocess.run(\n",
    "            [\"git\", \"rev-list\", \"--count\", \"HEAD\"],\n",
    "            cwd=repo_path, capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return int(out.stdout.strip())\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def readme_word_count(repo_path):\n",
    "    try:\n",
    "        for f in os.listdir(repo_path):\n",
    "            if f.lower().startswith(\"readme\"):\n",
    "                with open(os.path.join(repo_path, f), \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                    return len(re.findall(r\"\\b\\w+\\b\", fh.read()))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "def extract_basic_ml_features(repo_path):\n",
    "    \"\"\"\n",
    "    Build a simple ML feature dict similar to what the training script expects:\n",
    "    counts of common extensions, readme words, commit counts, directory depth, presence flags.\n",
    "    (Matches roughly the features you used earlier.)\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'num_commits': 0, 'readme_word_count': 0, 'directory_depth': 0,\n",
    "        'has_test_folder': 0, 'has_eslint': 0, 'has_dockerfile': 0,\n",
    "        'has_license': 0, 'has_gitignore': 0, 'has_package_json': 0,\n",
    "        'has_pom_xml': 0, 'has_requirements_txt': 0,\n",
    "        'count_py': 0, 'count_js': 0, 'count_md': 0, 'count_json': 0,\n",
    "        'count_html': 0, 'count_css': 0, 'count_java': 0, 'count_ts': 0,\n",
    "        'count_go': 0, 'count_rb': 0, 'count_php': 0,\n",
    "    }\n",
    "    features['num_commits'] = git_commit_count(repo_path)\n",
    "    features['readme_word_count'] = readme_word_count(repo_path)\n",
    "    max_depth = 0\n",
    "    test_folders = {'test', 'tests', 'spec', 'specs', '__tests__'}\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        # avoid .git internals\n",
    "        if \".git\" in root.split(os.sep):\n",
    "            continue\n",
    "        depth = root.replace(repo_path, \"\").count(os.sep)\n",
    "        if depth > max_depth: max_depth = depth\n",
    "        for d in dirs:\n",
    "            if d.lower() in test_folders:\n",
    "                features['has_test_folder'] = 1\n",
    "        for fname in files:\n",
    "            lower = fname.lower()\n",
    "            if lower == 'dockerfile': features['has_dockerfile'] = 1\n",
    "            if lower.startswith('.eslintrc'): features['has_eslint'] = 1\n",
    "            if lower.startswith('license'): features['has_license'] = 1\n",
    "            if lower == '.gitignore': features['has_gitignore'] = 1\n",
    "            if lower == 'package.json': features['has_package_json'] = 1\n",
    "            if lower == 'pom.xml': features['has_pom_xml'] = 1\n",
    "            if lower == 'requirements.txt': features['has_requirements_txt'] = 1\n",
    "            _, ext = os.path.splitext(lower)\n",
    "            if ext == '.py': features['count_py'] += 1\n",
    "            elif ext == '.js': features['count_js'] += 1\n",
    "            elif ext in ('.md', '.markdown'): features['count_md'] += 1\n",
    "            elif ext == '.json': features['count_json'] += 1\n",
    "            elif ext == '.html': features['count_html'] += 1\n",
    "            elif ext == '.css': features['count_css'] += 1\n",
    "            elif ext == '.java': features['count_java'] += 1\n",
    "            elif ext == '.ts': features['count_ts'] += 1\n",
    "            elif ext == '.go': features['count_go'] += 1\n",
    "            elif ext == '.rb': features['count_rb'] += 1\n",
    "            elif ext == '.php': features['count_php'] += 1\n",
    "    features['directory_depth'] = max_depth\n",
    "    return features\n",
    "\n",
    "def align_features_to_schema(feature_dict, schema_csv_path):\n",
    "    \"\"\"Read header of schema CSV and align feature dict into a dataframe row (missing -> 0).\"\"\"\n",
    "    if not os.path.exists(schema_csv_path):\n",
    "        # no schema: just return df with features present\n",
    "        return pd.DataFrame([feature_dict])\n",
    "    header = pd.read_csv(schema_csv_path, nrows=0).columns.tolist()\n",
    "    # training features = header minus NON_FEATURE_COLS found earlier in user's code\n",
    "    NON_FEATURE_COLS = ['repo_url','repo_name','tech_stack','tech_domain','quality_score']\n",
    "    training_features = [c for c in header if c not in NON_FEATURE_COLS]\n",
    "    df = pd.DataFrame([feature_dict])\n",
    "    df_aligned = df.reindex(columns=training_features, fill_value=0)\n",
    "    return df_aligned\n",
    "\n",
    "# --- Load artifacts if present ---\n",
    "print(\"Loading artifacts (if available)...\")\n",
    "models = {}\n",
    "for k, p in ARTIFACTS.items():\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            models[k] = joblib.load(p)\n",
    "            print(f\"  - loaded {p}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - failed to load {p}: {e}\")\n",
    "    else:\n",
    "        print(f\"  - not found: {p}\")\n",
    "\n",
    "# load prograde_model.joblib if present (may be saved ProGrade instance)\n",
    "prograde_model_obj = None\n",
    "if os.path.exists(PROGRADE_MODEL_PATH):\n",
    "    try:\n",
    "        prograde_model_obj = joblib.load(PROGRADE_MODEL_PATH)\n",
    "        print(f\"Loaded prograde_model from {PROGRADE_MODEL_PATH} (type: {type(prograde_model_obj)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {PROGRADE_MODEL_PATH}: {e}\")\n",
    "else:\n",
    "    print(f\"prograde_model.joblib not found at {PROGRADE_MODEL_PATH}\")\n",
    "\n",
    "# --- Get repo input from user in notebook ---\n",
    "repo_input = input(\"Enter a GitHub repo URL (https.. or git@.. or local path):\\n> \").strip()\n",
    "if not repo_input:\n",
    "    raise SystemExit(\"No input provided.\")\n",
    "\n",
    "# prepare temp clone if URL\n",
    "is_url = repo_input.startswith(\"http://\") or repo_input.startswith(\"https://\") or repo_input.startswith(\"git@\")\n",
    "tempdir = None\n",
    "repo_path = repo_input\n",
    "try:\n",
    "    if is_url:\n",
    "        tempdir = tempfile.mkdtemp(prefix=\"prograde_nb_clone_\")\n",
    "        print(\"Cloning (shallow) to temporary folder:\", tempdir)\n",
    "        try:\n",
    "            repo_obj = git.Repo.clone_from(repo_input, tempdir, depth=1)\n",
    "            # try to unshallow quickly: if fails we'll still have code for heuristic analysis\n",
    "            shallow_file = os.path.join(tempdir, \".git\", \"shallow\")\n",
    "            if os.path.exists(shallow_file):\n",
    "                try:\n",
    "                    repo_obj.git.fetch(\"--unshallow\")\n",
    "                except Exception:\n",
    "                    # ignore - we still have a working checkout\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # try full clone fallback\n",
    "            print(\"Shallow clone failed, trying full clone:\", e)\n",
    "            repo_obj = git.Repo.clone_from(repo_input, tempdir)\n",
    "        repo_path = tempdir\n",
    "    else:\n",
    "        # local path\n",
    "        if not os.path.exists(repo_input):\n",
    "            raise FileNotFoundError(f\"Local path does not exist: {repo_input}\")\n",
    "        repo_path = os.path.abspath(repo_input)\n",
    "\n",
    "    # 1) Heuristic analysis (repo name, tech stack, contributors, heuristic scores)\n",
    "    print(\"\\nRunning heuristic analyze_repository(...) (ProGrade core)...\")\n",
    "    try:\n",
    "        heuristic_report = analyze_repository(repo_path)\n",
    "    except Exception as e:\n",
    "        print(\"Heuristic analyze_repository raised:\", e)\n",
    "        heuristic_report = {}\n",
    "\n",
    "    # 2) If prograde_model object has analyze(repo_path), use it (this may be preferred)\n",
    "    model_report = None\n",
    "    if prograde_model_obj is not None and hasattr(prograde_model_obj, \"analyze\"):\n",
    "        try:\n",
    "            print(\"\\nCalling model.analyze(...) from prograde_model.joblib...\")\n",
    "            model_report = prograde_model_obj.analyze(repo_path)\n",
    "            print(\"Model analyze returned successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"model.analyze failed:\", e)\n",
    "            model_report = None\n",
    "\n",
    "    # 3) If separate ML artifacts present, perform ML predictions\n",
    "    ml_predictions = {}\n",
    "    if all(k in models for k in (\"quality_model\", \"domain_model\", \"domain_labels\")) and os.path.exists(ARTIFACTS[\"schema_csv\"]):\n",
    "        print(\"\\nRunning ML predictions (quality + domain) using separate artifacts...\")\n",
    "        feat_dict = extract_basic_ml_features(repo_path)\n",
    "        df_aligned = align_features_to_schema(feat_dict, ARTIFACTS[\"schema_csv\"])\n",
    "        try:\n",
    "            quality = models[\"quality_model\"].predict(df_aligned)[0]\n",
    "            domain_enc = models[\"domain_model\"].predict(df_aligned)\n",
    "            # domain_labels may be LabelEncoder saved\n",
    "            domain = models[\"domain_labels\"].inverse_transform(domain_enc)[0] if hasattr(models[\"domain_labels\"], \"inverse_transform\") else str(domain_enc)\n",
    "            ml_predictions = {\"quality_score\": float(quality), \"tech_domain_pred\": domain}\n",
    "            print(\"ML predictions ready.\")\n",
    "        except Exception as e:\n",
    "            print(\"ML prediction failed:\", e)\n",
    "            ml_predictions = {}\n",
    "\n",
    "    # --- Build final combined report ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ðŸš€ Analysis Report for: {repo_input}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Repo name (choose best available)\n",
    "    repo_name = heuristic_report.get(\"repo_name\") or (model_report.get(\"repo_name\") if model_report else None) \\\n",
    "                or os.path.basename(os.path.normpath(repo_path))\n",
    "    print(f\"\\nRepo Name: {repo_name}\")\n",
    "\n",
    "    # Tech Stack detected (heuristic preferred)\n",
    "    tech_stack = heuristic_report.get(\"tech_stack\") or (model_report.get(\"tech_stack\") if model_report else None)\n",
    "    print_tech = False\n",
    "    if tech_stack:\n",
    "        print(\"\\nTech Stack (Detected):\")\n",
    "        order = [\"languages\",\"frameworks\",\"databases\",\"other_tools\",\"ai_coding_assistants\",\"apis_and_services\"]\n",
    "        for cat in order:\n",
    "            items = tech_stack.get(cat) or tech_stack.get(cat.replace(\"other_tools\",\"apis_and_services\")) or []\n",
    "            if items:\n",
    "                print(f\"  --- {cat.replace('_',' ').title()} ---\")\n",
    "                for t in sorted(items):\n",
    "                    print(f\"    - {t}\")\n",
    "                print_tech = True\n",
    "    if not tech_stack or not print_tech:\n",
    "        print(\"\\nTech Stack (Detected):  - (none detected)\")\n",
    "\n",
    "    # Domain (prefer model_report, else ML separate)\n",
    "    domain = None\n",
    "    if model_report and model_report.get(\"domains\"):\n",
    "        domain = \", \".join(model_report.get(\"domains\"))\n",
    "    elif ml_predictions.get(\"tech_domain_pred\"):\n",
    "        domain = ml_predictions[\"tech_domain_pred\"]\n",
    "    else:\n",
    "        # heuristic can provide domains too\n",
    "        hdoms = heuristic_report.get(\"domains\")\n",
    "        if hdoms:\n",
    "            domain = \", \".join(hdoms)\n",
    "    print(f\"\\nDomain (Detected): {domain or '(unknown)'}\")\n",
    "\n",
    "    # Scores: prefer model_report 'scores' (all categories), fallback to heuristic scores or ml quality\n",
    "    scores = None\n",
    "    if model_report and model_report.get(\"scores\"):\n",
    "        scores = model_report[\"scores\"]\n",
    "    elif heuristic_report and heuristic_report.get(\"scores\"):\n",
    "        scores = heuristic_report[\"scores\"]\n",
    "    elif ml_predictions.get(\"quality_score\") is not None:\n",
    "        scores = {\"predicted_quality_score\": ml_predictions[\"quality_score\"]}\n",
    "    if scores:\n",
    "        print(\"\\nScores:\")\n",
    "        # if scores is dict print keys\n",
    "        if isinstance(scores, dict):\n",
    "            for k, v in scores.items():\n",
    "                # nice formatting\n",
    "                if isinstance(v, float):\n",
    "                    print(f\"  - {k.replace('_',' ').title()}: {v:.2f}\")\n",
    "                else:\n",
    "                    print(f\"  - {k.replace('_',' ').title()}: {v}\")\n",
    "        else:\n",
    "            print(\"  -\", scores)\n",
    "    else:\n",
    "        print(\"\\nScores: (none)\")\n",
    "\n",
    "    # Contributors: prefer model_report then heuristic\n",
    "    contributors = None\n",
    "    if model_report and model_report.get(\"contributors\"):\n",
    "        contributors = model_report[\"contributors\"]\n",
    "    elif heuristic_report and heuristic_report.get(\"contributors\"):\n",
    "        contributors = heuristic_report[\"contributors\"]\n",
    "    if contributors:\n",
    "        print(\"\\nTop Contributors:\")\n",
    "        # contributors may be list of dicts or strings\n",
    "        if isinstance(contributors, list) and contributors and isinstance(contributors[0], dict):\n",
    "            toshow = contributors if len(contributors)<=3 else contributors[:3]\n",
    "            for c in toshow:\n",
    "                print(f\"  - {c.get('name','unknown')}: {c.get('commits',0)} commit{'s' if c.get('commits',0)!=1 else ''}\")\n",
    "        else:\n",
    "            # fallback: print raw\n",
    "            for c in (contributors if len(contributors)<=3 else contributors[:3]):\n",
    "                print(f\"  - {c}\")\n",
    "    else:\n",
    "        print(\"\\nTop Contributors: (none)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "finally:\n",
    "    # cleanup\n",
    "    if 'tempdir' in locals() and tempdir and os.path.exists(tempdir):\n",
    "        try:\n",
    "            shutil.rmtree(tempdir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7a981-6581-489c-b61e-fdacf53ea649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bc1d6-2697-45e3-a8a6-df4f2bcefa13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
